{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QinfO-kQBwFP",
        "outputId": "938b7197-2563-4717-97fa-0b094e23e5f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-4t6wyx_4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-4t6wyx_4\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install opencv-python torch torchvision\n",
        "!pip install torch torchvision torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnnB65nPCIIa",
        "outputId": "ab569c6e-aab9-4f22-e203-2653d441f108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model downloaded and saved to sam_vit_b_01ec64.pth\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# URL for the model file\n",
        "url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\"\n",
        "\n",
        "# Local path to save the model\n",
        "model_path = \"sam_vit_b_01ec64.pth\"\n",
        "\n",
        "# Download the model\n",
        "response = requests.get(url)\n",
        "with open(model_path, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(\"Model downloaded and saved to\", model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0elB1sBDq0L",
        "outputId": "06cb10a2-b8d7-4387-fc81-5724c555ba02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (52.85.151.129)] [Connecting to r2u.stat.\r                                                                                                    \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                                                    \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [2 InRelease 12.7 kB/128 kB 10%] [3 InRelease 22.9 kB/129 kB 18%] [Waiting for headers] [Connecte\r                                                                                                    \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [2 InRelease 15.6 kB/128 kB 12%] [3 InRelease 34.4 kB/129 kB 27%] [4 InRelease 3,626 B/3,626 B 10\r0% [2 InRelease 15.6 kB/128 kB 12%] [3 InRelease 34.4 kB/129 kB 27%] [Connected to r2u.stat.illinois\r                                                                                                    \rHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Ign:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,308 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,097 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,585 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,150 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,181 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,440 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,330 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,583 kB]\n",
            "Fetched 24.9 MB in 3s (8,104 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libgl1-mesa-glx\n",
            "0 upgraded, 1 newly installed, 0 to remove and 67 not upgraded.\n",
            "Need to get 5,584 B of archives.\n",
            "After this operation, 74.8 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgl1-mesa-glx amd64 23.0.4-0ubuntu1~22.04.1 [5,584 B]\n",
            "Fetched 5,584 B in 0s (24.1 kB/s)\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "(Reading database ... 123599 files and directories currently installed.)\n",
            "Preparing to unpack .../libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y libgl1-mesa-glx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6dN0ztGEuOv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "c0lYHMW9EW-g",
        "outputId": "ce3d5fce-1af2-4c33-9056-2a582243a690"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c8ac537fc748>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/0002434fcecc427f805e7e8e4e63ad76.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mimage_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load the SAM model (using a smaller model like ViT-B)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from segment_anything import SamPredictor, sam_model_registry\n",
        "\n",
        "# Load the image\n",
        "img_path = '/content/0002434fcecc427f805e7e8e4e63ad76.jpg'\n",
        "image = cv2.imread(img_path)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Load the SAM model (using a smaller model like ViT-B)\n",
        "model_type = \"vit_b\"\n",
        "sam_checkpoint = \"sam_vit_b_01ec64.pth\"  # Ensure you use the right path\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the SAM model\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "# Create the predictor\n",
        "predictor = SamPredictor(sam)\n",
        "\n",
        "# Set the image in the predictor\n",
        "predictor.set_image(image_rgb)\n",
        "\n",
        "# Add more points to cover different regions of the sweater, especially near the edges\n",
        "input_points = np.array([[230, 140], [230, 300], [170, 250], [290, 250], [230, 380]])\n",
        "input_labels = np.array([1, 1, 1, 1, 1])  # All points indicate foreground (t-shirt)\n",
        "\n",
        "input_labels = np.array([1, 1, 1, 1, 1])  # Indicating all points belong to the foreground\n",
        "\n",
        "# Predict the mask for the sweater\n",
        "with torch.no_grad():\n",
        "    masks, _, _ = predictor.predict(\n",
        "        point_coords=input_points,\n",
        "        point_labels=input_labels,\n",
        "        multimask_output=False\n",
        "    )\n",
        "\n",
        "# Apply the mask to the image (focus on the sweater)\n",
        "mask = masks[0]  # Single mask\n",
        "segmented_image = cv2.bitwise_and(image, image, mask=mask.astype('uint8'))\n",
        "\n",
        "# Save or display the result\n",
        "cv2.imwrite(\"segmented_tshirt.png\", segmented_image)\n",
        "\n",
        "# Display the segmented image\n",
        "plt.imshow(cv2.cvtColor(segmented_image, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Segmented T-shirt\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hlUfO_TZ-6f",
        "outputId": "ed4f4f81-6362-46cd-c441-01c65938bfbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation completed for all images.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from segment_anything import SamPredictor, sam_model_registry\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the SAM model\n",
        "model_type = \"vit_b\"\n",
        "sam_checkpoint = '/content/sam_vit_b_01ec64.pth'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the SAM model\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "# Create the predictor # Moved this line outside the function\n",
        "predictor = SamPredictor(sam)\n",
        "\n",
        "# Directory containing the images to be segmented\n",
        "input_directory = '/content/drive/MyDrive/AAA'\n",
        "output_directory = '/content/drive/MyDrive/AAB'\n",
        "\n",
        "# Ensure output directory exists\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "# Function to calculate standardized points based on image size\n",
        "def get_standardized_points(image_shape):\n",
        "    height, width, _ = image_shape\n",
        "    points = ([[227,98], [230,507], [10,500], [269,964], [553,431]])\n",
        "    return np.array(points)\n",
        "\n",
        "# Function to segment an image\n",
        "def segment_image(image_path, output_path):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Set the image in the predictor\n",
        "    predictor.set_image(image_rgb)\n",
        "\n",
        "    # Calculate standardized points based on image size\n",
        "    input_points = get_standardized_points(image.shape)\n",
        "    input_labels = np.array([1] * len(input_points))  # All points belong to the foreground\n",
        "\n",
        "    # Predict the mask\n",
        "    with torch.no_grad():\n",
        "        masks, _, _ = predictor.predict(\n",
        "            point_coords=input_points,\n",
        "            point_labels=input_labels,\n",
        "            multimask_output=False\n",
        "        )\n",
        "\n",
        "    # Apply the mask to the image\n",
        "    mask = masks[0]  # Single mask\n",
        "    segmented_image = cv2.bitwise_and(image, image, mask=mask.astype('uint8'))\n",
        "\n",
        "    # Save the segmented image\n",
        "    cv2.imwrite(output_path, segmented_image)\n",
        "\n",
        "# List of images to process\n",
        "image_files = [f for f in os.listdir(input_directory) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# Loop through each image and apply segmentation\n",
        "for image_file in image_files:\n",
        "    input_path = os.path.join(input_directory, image_file)\n",
        "    output_path = os.path.join(output_directory, f'segmented_{image_file}')\n",
        "\n",
        "    # Segment the image\n",
        "    segment_image(input_path, output_path)\n",
        "\n",
        "print(\"Segmentation completed for all images.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}