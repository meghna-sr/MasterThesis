{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QinfO-kQBwFP"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install opencv-python torch torchvision\n",
        "!pip install torch torchvision torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnnB65nPCIIa"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# URL for the model file\n",
        "url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\"\n",
        "\n",
        "# Local path to save the model\n",
        "model_path = \"sam_vit_b_01ec64.pth\"\n",
        "\n",
        "# Download the model\n",
        "response = requests.get(url)\n",
        "with open(model_path, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(\"Model downloaded and saved to\", model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0elB1sBDq0L"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y libgl1-mesa-glx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6dN0ztGEuOv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0lYHMW9EW-g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from segment_anything import SamPredictor, sam_model_registry\n",
        "\n",
        "# Load the image\n",
        "img_path = '/content/0002434fcecc427f805e7e8e4e63ad76.jpg'\n",
        "image = cv2.imread(img_path)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Load the SAM model (using a smaller model like ViT-B)\n",
        "model_type = \"vit_b\"\n",
        "sam_checkpoint = \"sam_vit_b_01ec64.pth\"  # Ensure you use the right path\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the SAM model\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "# Create the predictor\n",
        "predictor = SamPredictor(sam)\n",
        "\n",
        "# Set the image in the predictor\n",
        "predictor.set_image(image_rgb)\n",
        "\n",
        "# Add more points to cover different regions of the sweater, especially near the edges\n",
        "input_points = np.array([[230, 140], [230, 300], [170, 250], [290, 250], [230, 380]])\n",
        "input_labels = np.array([1, 1, 1, 1, 1])  # All points indicate foreground (t-shirt)\n",
        "\n",
        "input_labels = np.array([1, 1, 1, 1, 1])  # Indicating all points belong to the foreground\n",
        "\n",
        "# Predict the mask for the sweater\n",
        "with torch.no_grad():\n",
        "    masks, _, _ = predictor.predict(\n",
        "        point_coords=input_points,\n",
        "        point_labels=input_labels,\n",
        "        multimask_output=False\n",
        "    )\n",
        "\n",
        "# Apply the mask to the image (focus on the sweater)\n",
        "mask = masks[0]  # Single mask\n",
        "segmented_image = cv2.bitwise_and(image, image, mask=mask.astype('uint8'))\n",
        "\n",
        "# Save or display the result\n",
        "cv2.imwrite(\"segmented_tshirt.png\", segmented_image)\n",
        "\n",
        "# Display the segmented image\n",
        "plt.imshow(cv2.cvtColor(segmented_image, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Segmented T-shirt\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hlUfO_TZ-6f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from segment_anything import SamPredictor, sam_model_registry\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the SAM model\n",
        "model_type = \"vit_b\"\n",
        "sam_checkpoint = '/content/sam_vit_b_01ec64.pth'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the SAM model\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "# Create the predictor # Moved this line outside the function\n",
        "predictor = SamPredictor(sam)\n",
        "\n",
        "# Directory containing the images to be segmented\n",
        "input_directory = '/content/drive/MyDrive/AAA'\n",
        "output_directory = '/content/drive/MyDrive/AAB'\n",
        "\n",
        "# Ensure output directory exists\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "# Function to calculate standardized points based on image size\n",
        "def get_standardized_points(image_shape):\n",
        "    height, width, _ = image_shape\n",
        "    points = ([[227,98], [230,507], [10,500], [269,964], [553,431]])\n",
        "    return np.array(points)\n",
        "\n",
        "# Function to segment an image\n",
        "def segment_image(image_path, output_path):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Set the image in the predictor\n",
        "    predictor.set_image(image_rgb)\n",
        "\n",
        "    # Calculate standardized points based on image size\n",
        "    input_points = get_standardized_points(image.shape)\n",
        "    input_labels = np.array([1] * len(input_points))  # All points belong to the foreground\n",
        "\n",
        "    # Predict the mask\n",
        "    with torch.no_grad():\n",
        "        masks, _, _ = predictor.predict(\n",
        "            point_coords=input_points,\n",
        "            point_labels=input_labels,\n",
        "            multimask_output=False\n",
        "        )\n",
        "\n",
        "    # Apply the mask to the image\n",
        "    mask = masks[0]  # Single mask\n",
        "    segmented_image = cv2.bitwise_and(image, image, mask=mask.astype('uint8'))\n",
        "\n",
        "    # Save the segmented image\n",
        "    cv2.imwrite(output_path, segmented_image)\n",
        "\n",
        "# List of images to process\n",
        "image_files = [f for f in os.listdir(input_directory) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# Loop through each image and apply segmentation\n",
        "for image_file in image_files:\n",
        "    input_path = os.path.join(input_directory, image_file)\n",
        "    output_path = os.path.join(output_directory, f'segmented_{image_file}')\n",
        "\n",
        "    # Segment the image\n",
        "    segment_image(input_path, output_path)\n",
        "\n",
        "print(\"Segmentation completed for all images.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}