{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"500fcef1f6e84909bc554236828b229f","deepnote_cell_type":"text-cell-h1"},"source":"# Set up","block_group":"db448b641ad04e3cb1254a8dd9bea423"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"70f6b40271414611bfc05223461888c6","deepnote_cell_type":"text-cell-h3"},"source":"### Installations","block_group":"ec3e0065f3d64978a5fdfc69e70b3b0a"},{"cell_type":"code","metadata":{"source_hash":"66a48c20","execution_start":1731631134708,"execution_millis":13481,"execution_context_id":"76ee8494-04d6-49d1-83a8-4526e29f83e1","deepnote_to_be_reexecuted":false,"cell_id":"59bfbe421e4b435aa5c8de6e56a863b4","deepnote_cell_type":"code"},"source":"!pip install transformers\n\n!pip install openpyxl==3.1.2\n\n!pip install requests\n","block_group":"c2fc2bfbcf89468eb5c0c5a52511465a","execution_count":1,"outputs":[{"name":"stdout","text":"Collecting transformers\n  Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from transformers) (1.23.4)\nCollecting pyyaml>=5.1\n  Downloading PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /shared-libs/python3.9/py/lib/python3.9/site-packages (from transformers) (3.8.0)\nCollecting huggingface-hub<1.0,>=0.23.2\n  Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /shared-libs/python3.9/py/lib/python3.9/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: packaging>=20.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from transformers) (21.3)\nRequirement already satisfied: regex!=2019.12.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from transformers) (2022.9.13)\nCollecting tokenizers<0.21,>=0.20\n  Downloading tokenizers-0.20.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from transformers) (4.64.1)\nCollecting safetensors>=0.4.1\n  Downloading safetensors-0.4.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.1/436.1 kB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.4.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests->transformers) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests->transformers) (3.4)\nInstalling collected packages: safetensors, pyyaml, huggingface-hub, tokenizers, transformers\nSuccessfully installed huggingface-hub-0.26.2 pyyaml-6.0.2 safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.2\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting openpyxl==3.1.2\n  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting et-xmlfile\n  Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\nInstalling collected packages: et-xmlfile, openpyxl\nSuccessfully installed et-xmlfile-2.0.0 openpyxl-3.1.2\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: requests in /shared-libs/python3.9/py/lib/python3.9/site-packages (2.28.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests) (2.1.1)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/d4797fc5-cfdc-4987-a0a1-aff8e8747b29","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"b40557ba25e74c1a9cbc21284b18b569","deepnote_cell_type":"text-cell-h3"},"source":"### Load model and processor","block_group":"b2fa0c378d0b4800b370a1babd2c0019"},{"cell_type":"code","metadata":{"source_hash":"b63033b8","execution_start":1731631670908,"execution_millis":79412,"execution_context_id":"76ee8494-04d6-49d1-83a8-4526e29f83e1","deepnote_to_be_reexecuted":false,"cell_id":"f2e39ee761ff4616ad8171e7952d60b4","deepnote_cell_type":"code"},"source":"import transformers\nfrom transformers import AutoProcessor, Blip2ForConditionalGeneration\nimport torch\n\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n# by default `from_pretrained` loads the weights in float32 - keeping float32, else errors in text generation below\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float32)","block_group":"1e022a91beb94d96945f0058090d0bd3","execution_count":2,"outputs":[{"name":"stderr","text":"/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n2024-11-15 00:47:52.190173: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-11-15 00:47:52.303350: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-11-15 00:47:52.307804: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2024-11-15 00:47:52.307817: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2024-11-15 00:47:52.333115: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-11-15 00:47:52.808192: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2024-11-15 00:47:52.808249: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2024-11-15 00:47:52.808256: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\nDownloading shards: 100%|██████████| 2/2 [01:13<00:00, 36.85s/it]\nLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s]\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/6a2a1a8c-c82d-49ef-be24-5bd5d94f3516","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"bce18050","execution_start":1731631842616,"execution_millis":0,"execution_context_id":"76ee8494-04d6-49d1-83a8-4526e29f83e1","deepnote_to_be_reexecuted":false,"cell_id":"8f0a037adfaf4933bfe3ee5e6c2ca9ff","deepnote_cell_type":"code"},"source":"import torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)","block_group":"d04ba24156d349f0bb41d51b4fb774b9","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"Blip2ForConditionalGeneration(\n  (vision_model): Blip2VisionModel(\n    (embeddings): Blip2VisionEmbeddings(\n      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n    )\n    (encoder): Blip2Encoder(\n      (layers): ModuleList(\n        (0): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (1): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (2): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (3): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (4): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (5): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (6): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (7): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (8): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (9): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (10): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (11): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (12): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (13): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (14): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (15): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (16): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (17): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (18): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (19): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (20): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (21): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (22): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (23): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (24): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (25): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (26): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (27): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (28): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (29): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (30): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (31): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (32): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (33): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (34): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (35): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (36): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (37): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n        (38): Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n      )\n    )\n    (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n  )\n  (qformer): Blip2QFormerModel(\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (encoder): Blip2QFormerEncoder(\n      (layer): ModuleList(\n        (0): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (language_projection): Linear(in_features=768, out_features=2560, bias=True)\n  (language_model): OPTForCausalLM(\n    (model): OPTModel(\n      (decoder): OPTDecoder(\n        (embed_tokens): Embedding(50272, 2560, padding_idx=1)\n        (embed_positions): OPTLearnedPositionalEmbedding(2050, 2560)\n        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (layers): ModuleList(\n          (0): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (1): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (2): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (3): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (4): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (5): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (6): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (7): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (8): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (9): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (10): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (11): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (12): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (13): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (14): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (15): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (16): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (17): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (18): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (19): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (20): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (21): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (22): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (23): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (24): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (25): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (26): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (27): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (28): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (29): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (30): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n          (31): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (lm_head): Linear(in_features=2560, out_features=50272, bias=False)\n  )\n)"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/ae5ccaf3-c581-4130-9056-dd1aab7d5f50","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"316ef00f38b849b8aec3a1582cec97a3","deepnote_cell_type":"text-cell-h3"},"source":"### Functions for image paths","block_group":"27689bd0a57044c3b411900474909552"},{"cell_type":"code","metadata":{"source_hash":"431f4d9a","execution_start":1731870734518,"execution_millis":3819,"execution_context_id":"c80c9d5e-f276-461b-b141-91925dce1d11","deepnote_to_be_reexecuted":false,"cell_id":"87796c11e9de480cb5a873ec806cc4e9","deepnote_cell_type":"code"},"source":"##Functions to get dataset image paths and add them to a list\nimport os\n\n## AI DESIGNS\n\n# One-Pieces\ndef get_onepieces(folder_onepieces, extensions=['.png', '.jpg', '.jpeg']):\n    return [os.path.join(folder_onepieces, f) for f in os.listdir(folder_onepieces) if os.path.splitext(f)[1].lower() in extensions]\n\nfolder_onepieces = '/work/AI_Designs/AI_Designs_OnePiece'\nlist_onepiece = get_onepieces(folder_onepieces)\n\n# Skirts\ndef get_skirts(folder_skirts, extensions=['.png', '.jpg', '.jpeg']):\n    return [os.path.join(folder_skirts, f) for f in os.listdir(folder_skirts) if os.path.splitext(f)[1].lower() in extensions]\n\nfolder_skirts = '/work/AI_Designs/AI_Designs_BarbieSkirt'\nlist_skirts = get_skirts(folder_skirts)\n\n#Snowsuit\ndef get_snowsuits(folder_snowsuits, extensions=['.png', '.jpg', '.jpeg']):\n    return [os.path.join(folder_snowsuits, f) for f in os.listdir(folder_snowsuits) if os.path.splitext(f)[1].lower() in extensions]\n\nfolder_snowsuits = '/work/AI_Designs/AI_Designs_Snowsuit'\nlist_snowsuits = get_snowsuits(folder_snowsuits)\n\n#Tops\ndef get_tops(folder_tops, extensions=['.png', '.jpg', '.jpeg']):\n    return [os.path.join(folder_tops, f) for f in os.listdir(folder_tops) if os.path.splitext(f)[1].lower() in extensions]\n\nfolder_tops = '/work/AI_Designs/AI_Designs_Top'\nlist_tops = get_tops(folder_tops)\n\n## BASELINE IMAGES - 1000\n\n# One-Pieces\ndef get_b1000_onepieces(folder_b1000_onepieces, extensions=['.png', '.jpg', '.jpeg']):\n    return [os.path.join(folder_b1000_onepieces, f) for f in os.listdir(folder_b1000_onepieces) if os.path.splitext(f)[1].lower() in extensions]\n\nfolder_b1000_onepieces = '/work/Baseline_OnePiece_ONE' \nlist_b1000_onepiece = get_b1000_onepieces(folder_b1000_onepieces)\n\n# Skirts\ndef get_b1000_skirts(folder_b1000_skirts, extensions=['.png', '.jpg', '.jpeg']):\n    return [os.path.join(folder_b1000_skirts, f) for f in os.listdir(folder_b1000_skirts) if os.path.splitext(f)[1].lower() in extensions]\n\nfolder_b1000_skirts = '/work/Baseline_Skirt_ONE'\nlist_b1000_skirts = get_b1000_skirts(folder_b1000_skirts)\n\n#Snowsuit\ndef get_b1000_snowsuits(folder_b1000_snowsuits, extensions=['.png', '.jpg', '.jpeg']):\n    return [os.path.join(folder_b1000_snowsuits, f) for f in os.listdir(folder_b1000_snowsuits) if os.path.splitext(f)[1].lower() in extensions]\n\nfolder_b1000_snowsuits = '/work/Baseline_Snowsuit_ONE'\nlist_b1000_snowsuits = get_b1000_snowsuits(folder_b1000_snowsuits)\n\n#Tops\ndef get_b1000_tops(folder_b1000_tops, extensions=['.png', '.jpg', '.jpeg']):\n    return [os.path.join(folder_b1000_tops, f) for f in os.listdir(folder_b1000_tops) if os.path.splitext(f)[1].lower() in extensions]\n\nfolder_b1000_tops = '/work/Baseline_Oberteil_ONE'\nlist_b1000_tops = get_b1000_tops(folder_b1000_tops)\n","block_group":"12056579e76c4a50b5e478b146d3faea","execution_count":1,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/work/AI_Designs/AI_Designs_BarbieSkirt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [1], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_skirts, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(folder_skirts) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(f)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m extensions]\n\u001b[1;32m     17\u001b[0m folder_skirts \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/work/AI_Designs/AI_Designs_BarbieSkirt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 18\u001b[0m list_skirts \u001b[38;5;241m=\u001b[39m \u001b[43mget_skirts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_skirts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#Snowsuit\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_snowsuits\u001b[39m(folder_snowsuits, extensions\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n","Cell \u001b[0;32mIn [1], line 15\u001b[0m, in \u001b[0;36mget_skirts\u001b[0;34m(folder_skirts, extensions)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_skirts\u001b[39m(folder_skirts, extensions\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_skirts, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_skirts\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(f)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m extensions]\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/work/AI_Designs/AI_Designs_BarbieSkirt'"]}],"outputs_reference":"s3:deepnote-cell-outputs-production/1edc1be9-2b55-495e-9aff-be2ef496dd9b","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"3aed00e060b84d9a85d8d65121e8b402","deepnote_cell_type":"text-cell-h1"},"source":"# Content Novelty","block_group":"5b994ceebe5b46d48ff8fa3b176645dd"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"deepnote_app_block_visible":false,"cell_id":"f7dd859742784869946e8d3b3df75d17","deepnote_cell_type":"text-cell-h2"},"source":"## Load model and processor","block_group":"574e37faeaf34c4f8d5562a79bf3da01"},{"cell_type":"code","metadata":{"source_hash":"9eafe7d3","execution_start":1731631854864,"execution_millis":2741,"execution_context_id":"76ee8494-04d6-49d1-83a8-4526e29f83e1","deepnote_to_be_reexecuted":false,"cell_id":"2bb710a422174f9fa9f063c7590d9462","deepnote_cell_type":"code"},"source":"import transformers\nfrom transformers import AutoProcessor, Blip2ForConditionalGeneration\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float32)","block_group":"48426602cc69408e821c824bc81de32e","execution_count":4,"outputs":[{"name":"stderr","text":"Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/4e47e77c-ea5e-499c-9e38-5e6f5acc2ca5","content_dependencies":null},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"deepnote_app_block_visible":false,"cell_id":"f90ac69d2f9144808b460ecd7a9225e7","deepnote_cell_type":"text-cell-h2"},"source":"## Define function for generating text description","block_group":"d4a15d08842846978946cdadf63cbfaa"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1724621570753,"execution_millis":2344,"deepnote_to_be_reexecuted":false,"cell_id":"c70f8bd296754bd2a98cf13678f83f4b","deepnote_cell_type":"code"},"source":"from PIL import Image\nimport torch\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\nimport numpy as np\nfrom transformers import BertTokenizer, BertModel\n\n# Reuse function to generate text from images\ndef generate_text_from_images(image_list, processor, model, device, max_new_tokens=90, min_new_tokens=60):\n    results = []\n    for image_path in image_list:\n        # Load and preprocess image\n        image = Image.open(skirt).convert('RGB')\n        inputs = processor(image, return_tensors=\"pt\").to(device)\n\n        # Generate text\n        with torch.cuda.amp.autocast(enabled=False):  # Disabling autocasting for now\n            generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, min_new_tokens=min_new_tokens)\n\n        # Decode generated text\n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n        results.append(generated_text)\n    \n    return results\n\n# Initialize BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\n#Get Text Embeddings\ndef get_embeddings(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    # Use the mean of the last hidden state as the sentence embedding\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n    return embeddings\n\n# Reuse processing function\ndef process_and_save(image_list, image_category, processor, model, device, output_file_prefix):\n    # Generate descriptions\n    generated_texts = generate_text_from_images(image_list, processor, model, device)\n    df = pd.DataFrame({\"Image\": image_list, \"Generated Text\": generated_texts})\n    \n    # Save descriptions to Excel\n    output_file = f\"{text_descrtiptions}_{image_category}.xlsx\"\n    df.to_excel(output_file, index=False)\n    print(f\"Generated texts saved to {output_file}\")\n\n    # Compute embeddings\n    descriptions = df[\"Generated Text\"].tolist()\n    embeddings = [get_embeddings(desc) for desc in descriptions]\n    \n    return df, embeddings    ","block_group":"c775865029ec4caeb1fdcba7bad21ce1","execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'list_b_skirts' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [5], line 44\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Process list_skirts\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#texts_skirts = generate_text_from_images(list_skirts , processor, model, device)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m##df_skirts = pd.DataFrame({\"Image\": list_skirts, \"Generated Text\": texts_skirts})\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#df_skirts.to_excel(\"1108generated_texts_skirts.xlsx\", index=False)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Process Baseline_skirts\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m texts_baseline \u001b[38;5;241m=\u001b[39m generate_text_from_images(\u001b[43mlist_b_skirts\u001b[49m, processor, model, device)\n\u001b[1;32m     45\u001b[0m df_baseline \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m\"\u001b[39m: list_b_skirts, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Text\u001b[39m\u001b[38;5;124m\"\u001b[39m: texts_baseline})\n\u001b[1;32m     46\u001b[0m df_baseline\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2508generated_texts_baseline_skirts.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","\u001b[0;31mNameError\u001b[0m: name 'list_b_skirts' is not defined"]}],"outputs_reference":"s3:deepnote-cell-outputs-production/0c18b892-60a2-4936-bee8-f3bdb60fb5f8","content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"6789e9006aa6432ea0a8395ab090961f","deepnote_cell_type":"code"},"source":"##Call function to generate text from images\n\n## AI DESIGNS\n\n#One-Piece\ntops_df, tops_embeddings = process_and_save(\n    list_tops, \"tops\", processor, model, device, \"generated_texts\"\n)\n\n#Skirt\nskirts_df, skirts_embeddings = process_and_save(\n    list_skirts, \"skirts\", processor, model, device, \"generated_texts\"\n)\n\n#Snowsuit\nsnowsuits_df, tops_embeddings = process_and_save(\n    list_snowsuits, \"snowsuits\", processor, model, device, \"generated_texts\"\n)\n\n#Tops\ntops_df, tops_embeddings = process_and_save(\n    list_tops, \"tops\", processor, model, device, \"generated_texts\"\n)\n\n## BASELINE IMAGES\n\n#One-Piece\ntops_b_df, tops_embeddings = process_and_save(\n    list_b1000_tops, \"b_tops\", processor, model, device, \"generated_texts\"\n)\n\n#Skirt\nskirts_df, skirts_embeddings = process_and_save(\n    list_b1000_skirts, \"b_skirts\", processor, model, device, \"generated_texts\"\n)\n\n#Snowsuit\nsnowsuits_df, tops_embeddings = process_and_save(\n    list_b1000_snowsuits, \"b_snowsuits\", processor, model, device, \"generated_texts\"\n)\n\n#Tops\ntops_df, tops_embeddings = process_and_save(\n    list_b1000_tops, \"b_tops\", processor, model, device, \"generated_texts\"\n)","block_group":"33d00fe7fe51449abd5e20e76a966dee","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"deepnote_app_block_visible":false,"cell_id":"533895a354da483eb29f6f5c498b794f","deepnote_cell_type":"text-cell-h2"},"source":"## Calculate Content Novelty","block_group":"118c2ff23fd44c93b3d2199f256fc49c"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"152ee9aef6154d99ae73c0469c451a02","deepnote_cell_type":"text-cell-h3"},"source":"### One-Pieces","block_group":"94b3670dee8e4d1e81d0bce4db9492c1"},{"cell_type":"code","metadata":{"cell_id":"d2c5b92c15984f909751506405c2766f","deepnote_cell_type":"code"},"source":"from PIL import Image\nimport torch\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel\n\n# Define function to process and generate text from images with improved parameters\ndef generate_text_from_images(list_onepiece, processor, model, device, max_new_tokens=60, min_new_tokens=40, temperature=0.7, top_p=0.7, num_beams=4):\n    results = []\n    for onepiece in list_onepiece:\n        # Load and preprocess image\n        image = Image.open(onepiece).convert('RGB')\n        prompt = \"This image features\"\n        inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device)\n\n        # Generate text using beam search, temperature, and nucleus sampling\n        with torch.no_grad():\n            generated_ids = model.generate(\n                **inputs, \n                max_new_tokens=max_new_tokens, \n                min_new_tokens=min_new_tokens, \n                temperature=temperature, \n                top_p=top_p, \n                num_beams=num_beams\n            )\n\n        # Decode generated text\n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n        results.append(generated_text)\n    \n    return results\n\n# Initialize BERT tokenizer and model for embeddings\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\ndef get_embeddings(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n    return embeddings\n\n# Process images\ntexts = generate_text_from_images(list_onepiece, processor, model, device)\ndf = pd.DataFrame({\"Image\": list_onepiece, \"Generated Text\": texts})\n\n# Save descriptions to Excel\ndf.to_excel(\"generated_texts_onepiece.xlsx\", index=False)\n","block_group":"ec2e61cee98f48ec9eb78f29d75fa45a","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"d184f8ac36dc4ad18a6f0d17998a5d51","deepnote_cell_type":"text-cell-h3"},"source":"### Skirts","block_group":"9c55253d35944912bf6512879fe0fe6b"},{"cell_type":"code","metadata":{"source_hash":"b25bca2d","execution_start":1731632621108,"execution_millis":455699,"execution_context_id":"76ee8494-04d6-49d1-83a8-4526e29f83e1","deepnote_to_be_reexecuted":false,"cell_id":"3167f37e1f514ac7817d1c293548bc44","deepnote_cell_type":"code"},"source":"from PIL import Image\nimport torch\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel\n\n# Define function to process and generate text from images with improved parameters\ndef generate_text_from_images(list_skirts, processor, model, device, max_new_tokens=60, min_new_tokens=40, temperature=0.7, top_p=0.7, num_beams=4):\n    results = []\n    for skirt in list_skirts:\n        # Load and preprocess image\n        image = Image.open(skirt).convert('RGB')\n        prompt = \"This image features\"\n        inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device)\n\n        # Generate text using beam search, temperature, and nucleus sampling\n        with torch.no_grad():\n            generated_ids = model.generate(\n                **inputs, \n                max_new_tokens=max_new_tokens, \n                min_new_tokens=min_new_tokens, \n                temperature=temperature, \n                top_p=top_p, \n                num_beams=num_beams\n            )\n\n        # Decode generated text\n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n        results.append(generated_text)\n    \n    return results\n\n# Initialize BERT tokenizer and model for embeddings\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\ndef get_embeddings(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n    return embeddings\n\n# Process images\ntexts = generate_text_from_images(list_skirts, processor, model, device)\ndf = pd.DataFrame({\"Image\": list_skirts, \"Generated Text\": texts})\n\n# Save descriptions to Excel\ndf.to_excel(\"generated_texts_skirts.xlsx\", index=False)\n","block_group":"4109bf3682944ec6a3579560e04eb0ff","execution_count":8,"outputs":[{"name":"stderr","text":"/root/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/root/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=60) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `min_new_tokens` (=40) and `min_length`(=32) seem to have been set. `min_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/ccdb22b6-c140-4b55-adce-6f0a33f2843a","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"71d0506899a442efb88b6e48f78c8f6b","deepnote_cell_type":"text-cell-h3"},"source":"### Snowsuits","block_group":"3959bf58a70c43c488934c9f137cfca9"},{"cell_type":"code","metadata":{"allow_embed":false,"cell_id":"4dfccf820e964247952a28b3d1f63018","deepnote_cell_type":"code"},"source":"from PIL import Image\nimport torch\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel\n\n# Define function to process and generate text from images with improved parameters\ndef generate_text_from_images(list_snowsuits, processor, model, device, max_new_tokens=60, min_new_tokens=40, temperature=0.7, top_p=0.7, num_beams=4):\n    results = []\n    for snowsuit in list_snowsuits:\n        # Load and preprocess image\n        image = Image.open(snowsuit).convert('RGB')\n        prompt = \"This image features\"\n        inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device)\n\n        # Generate text using beam search, temperature, and nucleus sampling\n        with torch.no_grad():\n            generated_ids = model.generate(\n                **inputs, \n                max_new_tokens=max_new_tokens, \n                min_new_tokens=min_new_tokens, \n                temperature=temperature, \n                top_p=top_p, \n                num_beams=num_beams\n            )\n\n        # Decode generated text\n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n        results.append(generated_text)\n    \n    return results\n\n# Initialize BERT tokenizer and model for embeddings\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\ndef get_embeddings(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n    return embeddings\n\n# Process images\ntexts = generate_text_from_images(list_snowsuits, processor, model, device)\ndf = pd.DataFrame({\"Image\": list_snowsuits, \"Generated Text\": texts})\n\n# Save descriptions to Excel\ndf.to_excel(\"generated_texts_snowsuits.xlsx\", index=False)\n","block_group":"dd9795b430ff4f8e9ea6564e31e5a79c","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"c703e50f8c7a4d37a9e76fa9b53fc6f7","deepnote_cell_type":"text-cell-h3"},"source":"### Tops","block_group":"36a7170cbf714839bd87a58ace459dfc"},{"cell_type":"code","metadata":{"cell_id":"97bbbb8e92be46aa90e3c225addbf945","deepnote_cell_type":"code"},"source":"from PIL import Image\nimport torch\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel\n\n# Define function to process and generate text from images with improved parameters\ndef generate_text_from_images(list_tops, processor, model, device, max_new_tokens=60, min_new_tokens=40, temperature=0.7, top_p=0.7, num_beams=4):\n    results = []\n    for top in list_tops:\n        # Load and preprocess image\n        image = Image.open(top).convert('RGB')\n        prompt = \"This image features\"\n        inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device)\n\n        # Generate text using beam search, temperature, and nucleus sampling\n        with torch.no_grad():\n            generated_ids = model.generate(\n                **inputs, \n                max_new_tokens=max_new_tokens, \n                min_new_tokens=min_new_tokens, \n                temperature=temperature, \n                top_p=top_p, \n                num_beams=num_beams\n            )\n\n        # Decode generated text\n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n        results.append(generated_text)\n    \n    return results\n\n# Initialize BERT tokenizer and model for embeddings\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\ndef get_embeddings(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n    return embeddings\n\n# Process images\ntexts = generate_text_from_images(list_tops, processor, model, device)\ndf = pd.DataFrame({\"Image\": list_tops, \"Generated Text\": texts})\n\n# Save descriptions to Excel\ndf.to_excel(\"generated_texts_tops.xlsx\", index=False)\n","block_group":"67dbb481ffc44ce8b7e67a8db89ffb57","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"c202cc06ec5f4833ba5277a88cc97bbc","deepnote_cell_type":"text-cell-h1"},"source":"# Visual Novelty ","block_group":"ed454f9b7f994727940be885e299bf06"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"deepnote_app_block_visible":false,"cell_id":"53bbb8305b2149019e99470774b0c55a","deepnote_cell_type":"text-cell-h2"},"source":"## Load Model and processor","block_group":"302e2496da21404193ad03c13adbe4d3"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1726775552465,"execution_millis":1376,"deepnote_to_be_reexecuted":false,"cell_id":"b766b8cdfc2243228fceeb729055a45f","deepnote_cell_type":"code"},"source":"import os\nimport random\nfrom PIL import Image\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\nfrom transformers import AutoFeatureExtractor, Dinov2Model, AutoImageProcessor\nmodel_name = 'facebook/dinov2-base'\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = Dinov2Model.from_pretrained(model_name).eval()\n#feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)","block_group":"abfdf86e166d48039d96d0105c3f4728","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"2bcd7111cbf94eda8bbc20ab08fce3df","deepnote_cell_type":"text-cell-h2"},"source":"## Compute novelty score using cosine similarity","block_group":"a76f49a29e164442afee12c63d6ad2c9"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1726777673343,"execution_millis":285616,"deepnote_to_be_reexecuted":false,"cell_id":"3e1a277e889643feb69163dd8fa5d670","deepnote_cell_type":"code"},"source":"FEATURE_DIM = 768\n\ndef extract_visual_features(image_paths, processor, model, device):\n    visual_features = []\n    for image_path in image_paths:\n        try:\n         # Load and preprocess image\n         image = Image.open(image_path).convert('RGB')\n         # Resize image to a consistent size\n         image = image.resize((224, 224)) \n         inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n         # Extract visual features\n         with torch.no_grad():\n            features = model(**inputs).last_hidden_state.squeeze(0).cpu()\n\n        # Average over the patch dimension to get a single feature vector for each image\n         flattened_features = features.mean(dim=0)\n\n        # Padding\n         if flattened_features.shape[0] < FEATURE_DIM:\n                padded_features = torch.nn.functional.pad(flattened_features, (0, FEATURE_DIM - flattened_features.shape[0]))\n         else:\n                padded_features = flattened_features[:FEATURE_DIM]\n         visual_features.append(flattened_features)\n        except Exception as e:\n            print(f\"Error prepocessing {image_path}: {e}\")\n            continue\n    \n    return visual_features\n\n\n#Load baseline images\nbaseline_image_folder = 'Baseline_OnePiece/'\nbaseline_image_paths = [os.path.join(baseline_image_folder, f) for f in os.listdir(baseline_image_folder) if f.endswith('.jpg')]\n\n# Subsample baseline images\nnum_baseline_samples = 10000 \nbaseline_image_paths_sampled = random.sample(baseline_image_paths, min(num_baseline_samples, len(baseline_image_paths)))\n\n# Extract visual features from baseline images\nbaseline_visual_features = extract_visual_features(baseline_image_paths_sampled, processor, model, device)\n\n# Extract visual features from query images\nvisual_features_query = extract_visual_features(list_onepiece, processor, model, device)\n\n# Check if the features list is empty\nif not baseline_visual_features or not visual_features_query:\n    raise ValueError(\"Feature extraction failed for all images.\")\n\n# Compute distances or similarities with baseline\ndef compute_distances_with_baseline(query_features, baseline_features):\n    # Convert query features to numpy array\n    query_feature_matrix = torch.stack(query_features).numpy()\n    # Convert baseline features to numpy array\n    baseline_feature_matrix = torch.stack(baseline_features).numpy()\n\n    cosine_sims = cosine_similarity(query_feature_matrix, baseline_feature_matrix)\n    return cosine_sims\n\n# Compute distances or similarities with baseline\ncosine_sims_with_baseline = compute_distances_with_baseline(visual_features_query, baseline_visual_features)\n\n# Compute novelty scores against baseline\ndef compute_novelty(cosine_sims):\n    # Cosine similarity\n    novelty_scores_cosine = 1 - np.mean(cosine_sims, axis=1)\n    return novelty_scores_cosine\n\nvisual_novelty_onepiece_cosine_with_baseline = compute_novelty( cosine_sims_with_baseline)\n\n# Prepare data to export into an Excel\ndata = {\n    'Image Path': list_onepiece,\n    'Novelty Score (Cosine)': visual_novelty_onepiece_cosine_with_baseline,\n}\n\n# Create a DataFrame\ndf = pd.DataFrame(data)\n\n# Export to Excel\noutput_file = 'visual_novelty_onepieces.xlsx'\ndf.to_excel(output_file, index=False)","block_group":"fd2ad90e26e84cab80f55586480832d5","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"323ae9c38c7247358ed76889558dce5b","deepnote_cell_type":"text-cell-h1"},"source":"# Measure Complexity","block_group":"f02a8fa01b424eb5a36eada8a8b81d92"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"deepnote_app_block_visible":false,"cell_id":"42f68086433942c58a0b79dbdb2070e6","deepnote_cell_type":"text-cell-h2"},"source":"## HSV Entropy","block_group":"eb6c182e8a5f4f1ab7c4e0d7f6fac9fc"},{"cell_type":"code","metadata":{"source_hash":"69a723a8","execution_start":1731624396089,"execution_millis":653884,"execution_context_id":"9e5fa201-6198-46b7-b6d0-dbf57062e44f","deepnote_to_be_reexecuted":false,"cell_id":"6d8b6dff21f64b1f9c3205091d5e9f03","deepnote_cell_type":"code"},"source":"import numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom collections import Counter\nimport os\n\ndef calculate_shannon_entropy(image_array):\n    pixel_counts = Counter(image_array.flatten())\n    total_pixels = image_array.size\n    entropy = -sum((count / total_pixels) * np.log2(count / total_pixels) for count in pixel_counts.values() if count > 0)\n    return entropy\n\ndef calculate_hsv_entropy(image_path):\n    image = Image.open(image_path).convert('HSV')\n    image_array = np.array(image)\n    h_channel = image_array[:, :, 0]\n    s_channel = image_array[:, :, 1]\n    v_channel = image_array[:, :, 2]\n    \n    h_entropy = calculate_shannon_entropy(h_channel)\n    s_entropy = calculate_shannon_entropy(s_channel)\n    v_entropy = calculate_shannon_entropy(v_channel)\n    combined_entropy = np.mean([h_entropy, s_entropy, v_entropy])\n    \n    return h_entropy, s_entropy, v_entropy, combined_entropy\n\ndef calculate_baseline_statistics(baseline_folder):\n    entropies = {\"H\": [], \"S\": [], \"V\": [], \"Combined\": []}\n    \n    for filename in os.listdir(baseline_folder):\n        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n            image_path = os.path.join(baseline_folder, filename)\n            h_entropy, s_entropy, v_entropy, combined_entropy = calculate_hsv_entropy(image_path)\n            entropies[\"H\"].append(h_entropy)\n            entropies[\"S\"].append(s_entropy)\n            entropies[\"V\"].append(v_entropy)\n            entropies[\"Combined\"].append(combined_entropy)\n    \n    baseline_mean = {k: np.mean(v) for k, v in entropies.items()}\n    baseline_std = {k: np.std(v) for k, v in entropies.items()}\n    \n    return baseline_mean, baseline_std\n\ndef calculate_z_scores(entropies, baseline_mean, baseline_std):\n    z_scores = {\n        \"H\": (entropies[\"H\"] - baseline_mean[\"H\"]) / baseline_std[\"H\"],\n        \"S\": (entropies[\"S\"] - baseline_mean[\"S\"]) / baseline_std[\"S\"],\n        \"V\": (entropies[\"V\"] - baseline_mean[\"V\"]) / baseline_std[\"V\"],\n        \"Combined\": (entropies[\"Combined\"] - baseline_mean[\"Combined\"]) / baseline_std[\"Combined\"]\n    }\n    return z_scores\n\n# Paths to folders - Update with corresponsing paths for skirts, snowsuits, tops\ntarget_folder = '/work/AI_Designs/AI_Designs_OnePiece'  \nbaseline_folder = '/work/Baseline_OnePiece_ONE' \n\n# Calculate baseline statistics\nbaseline_mean, baseline_std = calculate_baseline_statistics(baseline_folder)\n\n# List to store results\nresults = []\n\n# Iterate over images in the target folder\nfor filename in os.listdir(target_folder):\n    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n        image_path = os.path.join(target_folder, filename)\n        h_entropy, s_entropy, v_entropy, combined_entropy = calculate_hsv_entropy(image_path)\n        \n        entropies = {\n            \"H\": h_entropy,\n            \"S\": s_entropy,\n            \"V\": v_entropy,\n            \"Combined\": combined_entropy\n        }\n        \n        z_scores = calculate_z_scores(entropies, baseline_mean, baseline_std)\n        \n        results.append({\n            \"Image\": filename,\n            \"Hue Entropy (bits)\": round(h_entropy, 4),\n            \"Saturation Entropy (bits)\": round(s_entropy, 4),\n            \"Value Entropy (bits)\": round(v_entropy, 4),\n            \"Combined HSV Entropy (bits)\": round(combined_entropy, 4),\n            \"Hue Z-Score\": round(z_scores[\"H\"], 4),\n            \"Saturation Z-Score\": round(z_scores[\"S\"], 4),\n            \"Value Z-Score\": round(z_scores[\"V\"], 4),\n            \"Combined Z-Score\": round(z_scores[\"Combined\"], 4)\n        })\n\n# Convert the results to a DataFrame\ndf = pd.DataFrame(results)\n\n# Save the DataFrame to an Excel file\noutput_file = 'complexity_onepieces.xlsx'\ndf.to_excel(output_file, index=False)\n","block_group":"31bef8106c3d4646bece1997f1bfb4c1","execution_count":2,"outputs":[{"name":"stdout","text":"Results saved to 1411_hsv_entropy_z_scores_results_bOberteil.xlsx\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/266d606f-9b82-4a2b-a87e-edb2bb47d7cf","content_dependencies":null},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"deepnote_app_block_visible":false,"cell_id":"a81d021f5005493b99d2f10709dd9e0d","deepnote_cell_type":"text-cell-h1"},"source":"# Scrape Images","block_group":"cc03d8a5f26b46a5a3a1a68ce5ecc746"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1724158432365,"execution_millis":2444,"deepnote_to_be_reexecuted":false,"cell_id":"6742cf870529408aa74e1ec6eb55db6b","deepnote_cell_type":"code"},"source":"pip install apify-client","block_group":"7d63304329964f9c9ebec1bfe1f64509","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting apify-client\n  Downloading apify_client-1.7.1-py3-none-any.whl (70 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting httpx>=0.25.0\n  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting apify-shared~=1.1.1\n  Downloading apify_shared-1.1.2-py3-none-any.whl (12 kB)\nRequirement already satisfied: certifi in /shared-libs/python3.9/py/lib/python3.9/site-packages (from httpx>=0.25.0->apify-client) (2022.9.24)\nCollecting httpcore==1.*\n  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: anyio in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from httpx>=0.25.0->apify-client) (3.6.2)\nRequirement already satisfied: sniffio in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from httpx>=0.25.0->apify-client) (1.3.0)\nRequirement already satisfied: idna in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from httpx>=0.25.0->apify-client) (3.4)\nCollecting h11<0.15,>=0.13\n  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: h11, apify-shared, httpcore, httpx, apify-client\nSuccessfully installed apify-client-1.7.1 apify-shared-1.1.2 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/b80bd07f-5b0a-431e-a395-794e9f4196ba","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1721982001762,"execution_millis":45627,"deepnote_to_be_reexecuted":true,"cell_id":"59fc5a406f0e4a17946f78322d3907c5","deepnote_cell_type":"code"},"source":"\nimport os\nimport json\nfrom apify_client import ApifyClient\n\n# Initialize the Apify client with your API token\napi_token = 'apify_api_bXONqF5EhaKgNAHDvCiHv2zc0J7zke2o55D3'\nclient = ApifyClient(api_token)\n\n# Define the input configuration for the scraper\ninput_data = {\n    \"searchUrl\": \"https://www.amazon.de/s?k=snowsuit+kids&crid=QVL4VLTFLGLM&sprefix=snowsuit+%2Caps%2C181&ref=nb_sb_ss_pltr-xclick_2_9\",\n    \"maxItems\": 20,  # Adjust the number of items as needed\n    \"proxy\": {\n        \"useApifyProxy\": True,\n        \"apifyProxyGroups\": [\"RESIDENTIAL\"]\n    },\n}\n\n# Run the scraper\nrun = client.actor(\"curious_coder/amazon-scraper\").call(run_input=input_data)\n\n# Fetch the results\ndataset_id = run[\"defaultDatasetId\"]\nitems = client.dataset(dataset_id).list_items().items\n\n# JSON file path\njson_file_path = 'amazon_snowsuits.json'\n\n# Write the results to the JSON file\nwith open(json_file_path, mode='w', encoding='utf-8') as file:\n    json.dump(items, file, ensure_ascii=False, indent=4)\n\nprint(f\"Data saved to {json_file_path}\")\n","block_group":"818f50cd5feb448d923d9afb823f2706","execution_count":null,"outputs":[{"name":"stdout","text":"Number of items fetched: 0\nData saved to asos_skirts.json\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/dc4daa7b-32a2-4813-a519-c78e1b96e95d","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=9f956f77-27e9-4ead-b449-10d39e88f021' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2024-11-17T19:55:10.245Z"},"deepnote_notebook_id":"bf7dbafd6c4641f9be8c3785c3ba6016"}}