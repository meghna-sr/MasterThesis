{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"500fcef1f6e84909bc554236828b229f","deepnote_cell_type":"text-cell-h1"},"source":"# Set up","block_group":"db448b641ad04e3cb1254a8dd9bea423"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"70f6b40271414611bfc05223461888c6","deepnote_cell_type":"text-cell-h3"},"source":"### Installations","block_group":"ec3e0065f3d64978a5fdfc69e70b3b0a"},{"cell_type":"code","metadata":{"source_hash":"66a48c20","execution_start":1731631134708,"execution_millis":13481,"execution_context_id":"76ee8494-04d6-49d1-83a8-4526e29f83e1","deepnote_to_be_reexecuted":false,"cell_id":"59bfbe421e4b435aa5c8de6e56a863b4","deepnote_cell_type":"code"},"source":"!pip install transformers\n\n!pip install openpyxl==3.1.2\n\n!pip install requests\n","block_group":"c2fc2bfbcf89468eb5c0c5a52511465a","execution_count":1,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"b40557ba25e74c1a9cbc21284b18b569","deepnote_cell_type":"text-cell-h3"},"source":"### Load model and processor","block_group":"b2fa0c378d0b4800b370a1babd2c0019"},{"cell_type":"code","metadata":{"source_hash":"b63033b8","execution_start":1731631670908,"execution_millis":79412,"execution_context_id":"76ee8494-04d6-49d1-83a8-4526e29f83e1","deepnote_to_be_reexecuted":false,"cell_id":"f2e39ee761ff4616ad8171e7952d60b4","deepnote_cell_type":"code"},"source":"import transformers\nfrom transformers import AutoProcessor, Blip2ForConditionalGeneration\nimport torch\n\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n# by default `from_pretrained` loads the weights in float32 - keeping float32, else errors in text generation below\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float32)","block_group":"1e022a91beb94d96945f0058090d0bd3","execution_count":2,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"bce18050","execution_start":1731631842616,"execution_millis":0,"execution_context_id":"76ee8494-04d6-49d1-83a8-4526e29f83e1","deepnote_to_be_reexecuted":false,"cell_id":"8f0a037adfaf4933bfe3ee5e6c2ca9ff","deepnote_cell_type":"code"},"source":"import torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)","block_group":"d04ba24156d349f0bb41d51b4fb774b9","execution_count":3,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"316ef00f38b849b8aec3a1582cec97a3","deepnote_cell_type":"text-cell-h3"},"source":"### Functions for image paths","block_group":"27689bd0a57044c3b411900474909552"},{"cell_type":"code","metadata":{"source_hash":"431f4d9a","execution_start":1731870734518,"execution_millis":3819,"execution_context_id":"c80c9d5e-f276-461b-b141-91925dce1d11","deepnote_to_be_reexecuted":false,"cell_id":"87796c11e9de480cb5a873ec806cc4e9","deepnote_cell_type":"code"},"source":"##Functions to get dataset image paths and add them to a list\nimport os\n\n## AI DESIGNS\n\n# One-Pieces\ndef get_onepieces(folder_onepieces, extensions=['.png', '.jpg', '.jpeg']):\n    return [os.path.join(folder_onepieces, f) for f in os.listdir(folder_onepieces) if os.path.splitext(f)[1].lower() in extensions]\n\nfolder_onepieces = '/work/AI_Designs/AI_Designs_OnePiece'\nlist_onepiece = get_onepieces(folder_onepieces)\n\n# Skirts\ndef get_skirts(folder_skirts, extensions=['.png', '.jpg', '.jpeg']):\n    return [os.path.join(folder_skirts, f) for f in os.listdir(folder_skirts) if os.path.splitext(f)[1].lower() in extensions]\n\nfolder_skirts = '/work/AI_Designs/AI_Designs_BarbieSkirt'\nlist_skirts = get_skirts(folder_skirts)\n\n#Snowsuit\ndef get_snowsuits(folder_snowsuits, extensions=['.png', '.jpg', '.jpeg']):\n    return [os.path.join(folder_snowsuits, f) for f in os.listdir(folder_snowsuits) if os.path.splitext(f)[1].lower() in extensions]\n\nfolder_snowsuits = '/work/AI_Designs/AI_Designs_Snowsuit'\nlist_snowsuits = get_snowsuits(folder_snowsuits)\n\n#Tops\ndef get_tops(folder_tops, extensions=['.png', '.jpg', '.jpeg']):\n    return [os.path.join(folder_tops, f) for f in os.listdir(folder_tops) if os.path.splitext(f)[1].lower() in extensions]\n\nfolder_tops = '/work/AI_Designs/AI_Designs_Top'\nlist_tops = get_tops(folder_tops)\n\n## BASELINE IMAGES - 1000\n\n# One-Pieces\ndef get_b1000_onepieces(folder_b1000_onepieces, extensions=['.png', '.jpg', '.jpeg']):\n    return [os.path.join(folder_b1000_onepieces, f) for f in os.listdir(folder_b1000_onepieces) if os.path.splitext(f)[1].lower() in extensions]\n\nfolder_b1000_onepieces = '/work/Baseline_OnePiece_ONE' \nlist_b1000_onepiece = get_b1000_onepieces(folder_b1000_onepieces)\n\n# Skirts\ndef get_b1000_skirts(folder_b1000_skirts, extensions=['.png', '.jpg', '.jpeg']):\n    return [os.path.join(folder_b1000_skirts, f) for f in os.listdir(folder_b1000_skirts) if os.path.splitext(f)[1].lower() in extensions]\n\nfolder_b1000_skirts = '/work/Baseline_Skirt_ONE'\nlist_b1000_skirts = get_b1000_skirts(folder_b1000_skirts)\n\n#Snowsuit\ndef get_b1000_snowsuits(folder_b1000_snowsuits, extensions=['.png', '.jpg', '.jpeg']):\n    return [os.path.join(folder_b1000_snowsuits, f) for f in os.listdir(folder_b1000_snowsuits) if os.path.splitext(f)[1].lower() in extensions]\n\nfolder_b1000_snowsuits = '/work/Baseline_Snowsuit_ONE'\nlist_b1000_snowsuits = get_b1000_snowsuits(folder_b1000_snowsuits)\n\n#Tops\ndef get_b1000_tops(folder_b1000_tops, extensions=['.png', '.jpg', '.jpeg']):\n    return [os.path.join(folder_b1000_tops, f) for f in os.listdir(folder_b1000_tops) if os.path.splitext(f)[1].lower() in extensions]\n\nfolder_b1000_tops = '/work/Baseline_Oberteil_ONE'\nlist_b1000_tops = get_b1000_tops(folder_b1000_tops)\n","block_group":"12056579e76c4a50b5e478b146d3faea","execution_count":1,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"3aed00e060b84d9a85d8d65121e8b402","deepnote_cell_type":"text-cell-h1"},"source":"# Content Novelty","block_group":"5b994ceebe5b46d48ff8fa3b176645dd"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"deepnote_app_block_visible":false,"cell_id":"f7dd859742784869946e8d3b3df75d17","deepnote_cell_type":"text-cell-h2"},"source":"## Load model and processor","block_group":"574e37faeaf34c4f8d5562a79bf3da01"},{"cell_type":"code","metadata":{"source_hash":"9eafe7d3","execution_start":1731631854864,"execution_millis":2741,"execution_context_id":"76ee8494-04d6-49d1-83a8-4526e29f83e1","deepnote_to_be_reexecuted":false,"cell_id":"2bb710a422174f9fa9f063c7590d9462","deepnote_cell_type":"code"},"source":"import transformers\nfrom transformers import AutoProcessor, Blip2ForConditionalGeneration\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float32)","block_group":"48426602cc69408e821c824bc81de32e","execution_count":4,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"deepnote_app_block_visible":false,"cell_id":"f90ac69d2f9144808b460ecd7a9225e7","deepnote_cell_type":"text-cell-h2"},"source":"## Define function for generating text description","block_group":"d4a15d08842846978946cdadf63cbfaa"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1724621570753,"execution_millis":2344,"deepnote_to_be_reexecuted":false,"cell_id":"c70f8bd296754bd2a98cf13678f83f4b","deepnote_cell_type":"code"},"source":"from PIL import Image\nimport torch\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\nimport numpy as np\nfrom transformers import BertTokenizer, BertModel\n\n# Reuse function to generate text from images\ndef generate_text_from_images(image_list, processor, model, device, max_new_tokens=90, min_new_tokens=60):\n    results = []\n    for image_path in image_list:\n        # Load and preprocess image\n        image = Image.open(skirt).convert('RGB')\n        inputs = processor(image, return_tensors=\"pt\").to(device)\n\n        # Generate text\n        with torch.cuda.amp.autocast(enabled=False):  # Disabling autocasting for now\n            generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, min_new_tokens=min_new_tokens)\n\n        # Decode generated text\n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n        results.append(generated_text)\n    \n    return results\n\n# Initialize BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\n#Get Text Embeddings\ndef get_embeddings(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    # Use the mean of the last hidden state as the sentence embedding\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n    return embeddings\n\n# Reuse processing function\ndef process_and_save(image_list, image_category, processor, model, device, output_file_prefix):\n    # Generate descriptions\n    generated_texts = generate_text_from_images(image_list, processor, model, device)\n    df = pd.DataFrame({\"Image\": image_list, \"Generated Text\": generated_texts})\n    \n    # Save descriptions to Excel\n    output_file = f\"{text_descrtiptions}_{image_category}.xlsx\"\n    df.to_excel(output_file, index=False)\n    print(f\"Generated texts saved to {output_file}\")\n\n    # Compute embeddings\n    descriptions = df[\"Generated Text\"].tolist()\n    embeddings = [get_embeddings(desc) for desc in descriptions]\n    \n    return df, embeddings    ","block_group":"c775865029ec4caeb1fdcba7bad21ce1","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"6789e9006aa6432ea0a8395ab090961f","deepnote_cell_type":"code"},"source":"##Call function to generate text from images\n\n## AI DESIGNS\n\n#One-Piece\ntops_df, tops_embeddings = process_and_save(\n    list_tops, \"tops\", processor, model, device, \"generated_texts\"\n)\n\n#Skirt\nskirts_df, skirts_embeddings = process_and_save(\n    list_skirts, \"skirts\", processor, model, device, \"generated_texts\"\n)\n\n#Snowsuit\nsnowsuits_df, tops_embeddings = process_and_save(\n    list_snowsuits, \"snowsuits\", processor, model, device, \"generated_texts\"\n)\n\n#Tops\ntops_df, tops_embeddings = process_and_save(\n    list_tops, \"tops\", processor, model, device, \"generated_texts\"\n)\n\n## BASELINE IMAGES\n\n#One-Piece\ntops_b_df, tops_embeddings = process_and_save(\n    list_b1000_tops, \"b_tops\", processor, model, device, \"generated_texts\"\n)\n\n#Skirt\nskirts_df, skirts_embeddings = process_and_save(\n    list_b1000_skirts, \"b_skirts\", processor, model, device, \"generated_texts\"\n)\n\n#Snowsuit\nsnowsuits_df, tops_embeddings = process_and_save(\n    list_b1000_snowsuits, \"b_snowsuits\", processor, model, device, \"generated_texts\"\n)\n\n#Tops\ntops_df, tops_embeddings = process_and_save(\n    list_b1000_tops, \"b_tops\", processor, model, device, \"generated_texts\"\n)","block_group":"33d00fe7fe51449abd5e20e76a966dee","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"deepnote_app_block_visible":false,"cell_id":"533895a354da483eb29f6f5c498b794f","deepnote_cell_type":"text-cell-h2"},"source":"## Calculate Content Novelty","block_group":"118c2ff23fd44c93b3d2199f256fc49c"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"152ee9aef6154d99ae73c0469c451a02","deepnote_cell_type":"text-cell-h3"},"source":"### One-Pieces","block_group":"94b3670dee8e4d1e81d0bce4db9492c1"},{"cell_type":"code","metadata":{"cell_id":"d2c5b92c15984f909751506405c2766f","deepnote_cell_type":"code"},"source":"from PIL import Image\nimport torch\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel\n\n# Define function to process and generate text from images with improved parameters\ndef generate_text_from_images(list_onepiece, processor, model, device, max_new_tokens=60, min_new_tokens=40, temperature=0.7, top_p=0.7, num_beams=4):\n    results = []\n    for onepiece in list_onepiece:\n        # Load and preprocess image\n        image = Image.open(onepiece).convert('RGB')\n        prompt = \"This image features\"\n        inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device)\n\n        # Generate text using beam search, temperature, and nucleus sampling\n        with torch.no_grad():\n            generated_ids = model.generate(\n                **inputs, \n                max_new_tokens=max_new_tokens, \n                min_new_tokens=min_new_tokens, \n                temperature=temperature, \n                top_p=top_p, \n                num_beams=num_beams\n            )\n\n        # Decode generated text\n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n        results.append(generated_text)\n    \n    return results\n\n# Initialize BERT tokenizer and model for embeddings\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\ndef get_embeddings(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n    return embeddings\n\n# Process images\ntexts = generate_text_from_images(list_onepiece, processor, model, device)\ndf = pd.DataFrame({\"Image\": list_onepiece, \"Generated Text\": texts})\n\n# Save descriptions to Excel\ndf.to_excel(\"generated_texts_onepiece.xlsx\", index=False)\n","block_group":"ec2e61cee98f48ec9eb78f29d75fa45a","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"d184f8ac36dc4ad18a6f0d17998a5d51","deepnote_cell_type":"text-cell-h3"},"source":"### Skirts","block_group":"9c55253d35944912bf6512879fe0fe6b"},{"cell_type":"code","metadata":{"source_hash":"b25bca2d","execution_start":1731632621108,"execution_millis":455699,"execution_context_id":"76ee8494-04d6-49d1-83a8-4526e29f83e1","deepnote_to_be_reexecuted":false,"cell_id":"3167f37e1f514ac7817d1c293548bc44","deepnote_cell_type":"code"},"source":"from PIL import Image\nimport torch\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel\n\n# Define function to process and generate text from images with improved parameters\ndef generate_text_from_images(list_skirts, processor, model, device, max_new_tokens=60, min_new_tokens=40, temperature=0.7, top_p=0.7, num_beams=4):\n    results = []\n    for skirt in list_skirts:\n        # Load and preprocess image\n        image = Image.open(skirt).convert('RGB')\n        prompt = \"This image features\"\n        inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device)\n\n        # Generate text using beam search, temperature, and nucleus sampling\n        with torch.no_grad():\n            generated_ids = model.generate(\n                **inputs, \n                max_new_tokens=max_new_tokens, \n                min_new_tokens=min_new_tokens, \n                temperature=temperature, \n                top_p=top_p, \n                num_beams=num_beams\n            )\n\n        # Decode generated text\n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n        results.append(generated_text)\n    \n    return results\n\n# Initialize BERT tokenizer and model for embeddings\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\ndef get_embeddings(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n    return embeddings\n\n# Process images\ntexts = generate_text_from_images(list_skirts, processor, model, device)\ndf = pd.DataFrame({\"Image\": list_skirts, \"Generated Text\": texts})\n\n# Save descriptions to Excel\ndf.to_excel(\"generated_texts_skirts.xlsx\", index=False)\n","block_group":"4109bf3682944ec6a3579560e04eb0ff","execution_count":8,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"71d0506899a442efb88b6e48f78c8f6b","deepnote_cell_type":"text-cell-h3"},"source":"### Snowsuits","block_group":"3959bf58a70c43c488934c9f137cfca9"},{"cell_type":"code","metadata":{"allow_embed":false,"cell_id":"4dfccf820e964247952a28b3d1f63018","deepnote_cell_type":"code"},"source":"from PIL import Image\nimport torch\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel\n\n# Define function to process and generate text from images with improved parameters\ndef generate_text_from_images(list_snowsuits, processor, model, device, max_new_tokens=60, min_new_tokens=40, temperature=0.7, top_p=0.7, num_beams=4):\n    results = []\n    for snowsuit in list_snowsuits:\n        # Load and preprocess image\n        image = Image.open(snowsuit).convert('RGB')\n        prompt = \"This image features\"\n        inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device)\n\n        # Generate text using beam search, temperature, and nucleus sampling\n        with torch.no_grad():\n            generated_ids = model.generate(\n                **inputs, \n                max_new_tokens=max_new_tokens, \n                min_new_tokens=min_new_tokens, \n                temperature=temperature, \n                top_p=top_p, \n                num_beams=num_beams\n            )\n\n        # Decode generated text\n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n        results.append(generated_text)\n    \n    return results\n\n# Initialize BERT tokenizer and model for embeddings\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\ndef get_embeddings(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n    return embeddings\n\n# Process images\ntexts = generate_text_from_images(list_snowsuits, processor, model, device)\ndf = pd.DataFrame({\"Image\": list_snowsuits, \"Generated Text\": texts})\n\n# Save descriptions to Excel\ndf.to_excel(\"generated_texts_snowsuits.xlsx\", index=False)\n","block_group":"dd9795b430ff4f8e9ea6564e31e5a79c","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"c703e50f8c7a4d37a9e76fa9b53fc6f7","deepnote_cell_type":"text-cell-h3"},"source":"### Tops","block_group":"36a7170cbf714839bd87a58ace459dfc"},{"cell_type":"code","metadata":{"cell_id":"97bbbb8e92be46aa90e3c225addbf945","deepnote_cell_type":"code"},"source":"from PIL import Image\nimport torch\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel\n\n# Define function to process and generate text from images with improved parameters\ndef generate_text_from_images(list_tops, processor, model, device, max_new_tokens=60, min_new_tokens=40, temperature=0.7, top_p=0.7, num_beams=4):\n    results = []\n    for top in list_tops:\n        # Load and preprocess image\n        image = Image.open(top).convert('RGB')\n        prompt = \"This image features\"\n        inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device)\n\n        # Generate text using beam search, temperature, and nucleus sampling\n        with torch.no_grad():\n            generated_ids = model.generate(\n                **inputs, \n                max_new_tokens=max_new_tokens, \n                min_new_tokens=min_new_tokens, \n                temperature=temperature, \n                top_p=top_p, \n                num_beams=num_beams\n            )\n\n        # Decode generated text\n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n        results.append(generated_text)\n    \n    return results\n\n# Initialize BERT tokenizer and model for embeddings\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\ndef get_embeddings(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n    return embeddings\n\n# Process images\ntexts = generate_text_from_images(list_tops, processor, model, device)\ndf = pd.DataFrame({\"Image\": list_tops, \"Generated Text\": texts})\n\n# Save descriptions to Excel\ndf.to_excel(\"generated_texts_tops.xlsx\", index=False)\n","block_group":"67dbb481ffc44ce8b7e67a8db89ffb57","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"c202cc06ec5f4833ba5277a88cc97bbc","deepnote_cell_type":"text-cell-h1"},"source":"# Visual Novelty ","block_group":"ed454f9b7f994727940be885e299bf06"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"deepnote_app_block_visible":false,"cell_id":"53bbb8305b2149019e99470774b0c55a","deepnote_cell_type":"text-cell-h2"},"source":"## Load Model and processor","block_group":"302e2496da21404193ad03c13adbe4d3"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1726775552465,"execution_millis":1376,"deepnote_to_be_reexecuted":false,"cell_id":"b766b8cdfc2243228fceeb729055a45f","deepnote_cell_type":"code"},"source":"import os\nimport random\nfrom PIL import Image\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\nfrom transformers import AutoFeatureExtractor, Dinov2Model, AutoImageProcessor\nmodel_name = 'facebook/dinov2-base'\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = Dinov2Model.from_pretrained(model_name).eval()\n#feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)","block_group":"abfdf86e166d48039d96d0105c3f4728","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"2bcd7111cbf94eda8bbc20ab08fce3df","deepnote_cell_type":"text-cell-h2"},"source":"## Compute novelty score using cosine similarity","block_group":"a76f49a29e164442afee12c63d6ad2c9"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1726777673343,"execution_millis":285616,"deepnote_to_be_reexecuted":false,"cell_id":"3e1a277e889643feb69163dd8fa5d670","deepnote_cell_type":"code"},"source":"FEATURE_DIM = 768\n\ndef extract_visual_features(image_paths, processor, model, device):\n    visual_features = []\n    for image_path in image_paths:\n        try:\n         # Load and preprocess image\n         image = Image.open(image_path).convert('RGB')\n         # Resize image to a consistent size\n         image = image.resize((224, 224)) \n         inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n         # Extract visual features\n         with torch.no_grad():\n            features = model(**inputs).last_hidden_state.squeeze(0).cpu()\n\n        # Average over the patch dimension to get a single feature vector for each image\n         flattened_features = features.mean(dim=0)\n\n        # Padding\n         if flattened_features.shape[0] < FEATURE_DIM:\n                padded_features = torch.nn.functional.pad(flattened_features, (0, FEATURE_DIM - flattened_features.shape[0]))\n         else:\n                padded_features = flattened_features[:FEATURE_DIM]\n         visual_features.append(flattened_features)\n        except Exception as e:\n            print(f\"Error prepocessing {image_path}: {e}\")\n            continue\n    \n    return visual_features\n\n\n#Load baseline images\nbaseline_image_folder = 'Baseline_OnePiece/'\nbaseline_image_paths = [os.path.join(baseline_image_folder, f) for f in os.listdir(baseline_image_folder) if f.endswith('.jpg')]\n\n# Subsample baseline images\nnum_baseline_samples = 10000 \nbaseline_image_paths_sampled = random.sample(baseline_image_paths, min(num_baseline_samples, len(baseline_image_paths)))\n\n# Extract visual features from baseline images\nbaseline_visual_features = extract_visual_features(baseline_image_paths_sampled, processor, model, device)\n\n# Extract visual features from query images\nvisual_features_query = extract_visual_features(list_onepiece, processor, model, device)\n\n# Check if the features list is empty\nif not baseline_visual_features or not visual_features_query:\n    raise ValueError(\"Feature extraction failed for all images.\")\n\n# Compute distances or similarities with baseline\ndef compute_distances_with_baseline(query_features, baseline_features):\n    # Convert query features to numpy array\n    query_feature_matrix = torch.stack(query_features).numpy()\n    # Convert baseline features to numpy array\n    baseline_feature_matrix = torch.stack(baseline_features).numpy()\n\n    cosine_sims = cosine_similarity(query_feature_matrix, baseline_feature_matrix)\n    return cosine_sims\n\n# Compute distances or similarities with baseline\ncosine_sims_with_baseline = compute_distances_with_baseline(visual_features_query, baseline_visual_features)\n\n# Compute novelty scores against baseline\ndef compute_novelty(cosine_sims):\n    # Cosine similarity\n    novelty_scores_cosine = 1 - np.mean(cosine_sims, axis=1)\n    return novelty_scores_cosine\n\nvisual_novelty_onepiece_cosine_with_baseline = compute_novelty( cosine_sims_with_baseline)\n\n# Prepare data to export into an Excel\ndata = {\n    'Image Path': list_onepiece,\n    'Novelty Score (Cosine)': visual_novelty_onepiece_cosine_with_baseline,\n}\n\n# Create a DataFrame\ndf = pd.DataFrame(data)\n\n# Export to Excel\noutput_file = 'visual_novelty_onepieces.xlsx'\ndf.to_excel(output_file, index=False)","block_group":"fd2ad90e26e84cab80f55586480832d5","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"323ae9c38c7247358ed76889558dce5b","deepnote_cell_type":"text-cell-h1"},"source":"# Measure Complexity","block_group":"f02a8fa01b424eb5a36eada8a8b81d92"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"deepnote_app_block_visible":false,"cell_id":"42f68086433942c58a0b79dbdb2070e6","deepnote_cell_type":"text-cell-h2"},"source":"## HSV Entropy","block_group":"eb6c182e8a5f4f1ab7c4e0d7f6fac9fc"},{"cell_type":"code","metadata":{"source_hash":"69a723a8","execution_start":1731624396089,"execution_millis":653884,"execution_context_id":"9e5fa201-6198-46b7-b6d0-dbf57062e44f","deepnote_to_be_reexecuted":false,"cell_id":"6d8b6dff21f64b1f9c3205091d5e9f03","deepnote_cell_type":"code"},"source":"import numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom collections import Counter\nimport os\n\ndef calculate_shannon_entropy(image_array):\n    pixel_counts = Counter(image_array.flatten())\n    total_pixels = image_array.size\n    entropy = -sum((count / total_pixels) * np.log2(count / total_pixels) for count in pixel_counts.values() if count > 0)\n    return entropy\n\ndef calculate_hsv_entropy(image_path):\n    image = Image.open(image_path).convert('HSV')\n    image_array = np.array(image)\n    h_channel = image_array[:, :, 0]\n    s_channel = image_array[:, :, 1]\n    v_channel = image_array[:, :, 2]\n    \n    h_entropy = calculate_shannon_entropy(h_channel)\n    s_entropy = calculate_shannon_entropy(s_channel)\n    v_entropy = calculate_shannon_entropy(v_channel)\n    combined_entropy = np.mean([h_entropy, s_entropy, v_entropy])\n    \n    return h_entropy, s_entropy, v_entropy, combined_entropy\n\ndef calculate_baseline_statistics(baseline_folder):\n    entropies = {\"H\": [], \"S\": [], \"V\": [], \"Combined\": []}\n    \n    for filename in os.listdir(baseline_folder):\n        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n            image_path = os.path.join(baseline_folder, filename)\n            h_entropy, s_entropy, v_entropy, combined_entropy = calculate_hsv_entropy(image_path)\n            entropies[\"H\"].append(h_entropy)\n            entropies[\"S\"].append(s_entropy)\n            entropies[\"V\"].append(v_entropy)\n            entropies[\"Combined\"].append(combined_entropy)\n    \n    baseline_mean = {k: np.mean(v) for k, v in entropies.items()}\n    baseline_std = {k: np.std(v) for k, v in entropies.items()}\n    \n    return baseline_mean, baseline_std\n\ndef calculate_z_scores(entropies, baseline_mean, baseline_std):\n    z_scores = {\n        \"H\": (entropies[\"H\"] - baseline_mean[\"H\"]) / baseline_std[\"H\"],\n        \"S\": (entropies[\"S\"] - baseline_mean[\"S\"]) / baseline_std[\"S\"],\n        \"V\": (entropies[\"V\"] - baseline_mean[\"V\"]) / baseline_std[\"V\"],\n        \"Combined\": (entropies[\"Combined\"] - baseline_mean[\"Combined\"]) / baseline_std[\"Combined\"]\n    }\n    return z_scores\n\n# Paths to folders - Update with corresponsing paths for skirts, snowsuits, tops\ntarget_folder = '/work/AI_Designs/AI_Designs_OnePiece'  \nbaseline_folder = '/work/Baseline_OnePiece_ONE' \n\n# Calculate baseline statistics\nbaseline_mean, baseline_std = calculate_baseline_statistics(baseline_folder)\n\n# List to store results\nresults = []\n\n# Iterate over images in the target folder\nfor filename in os.listdir(target_folder):\n    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n        image_path = os.path.join(target_folder, filename)\n        h_entropy, s_entropy, v_entropy, combined_entropy = calculate_hsv_entropy(image_path)\n        \n        entropies = {\n            \"H\": h_entropy,\n            \"S\": s_entropy,\n            \"V\": v_entropy,\n            \"Combined\": combined_entropy\n        }\n        \n        z_scores = calculate_z_scores(entropies, baseline_mean, baseline_std)\n        \n        results.append({\n            \"Image\": filename,\n            \"Hue Entropy (bits)\": round(h_entropy, 4),\n            \"Saturation Entropy (bits)\": round(s_entropy, 4),\n            \"Value Entropy (bits)\": round(v_entropy, 4),\n            \"Combined HSV Entropy (bits)\": round(combined_entropy, 4),\n            \"Hue Z-Score\": round(z_scores[\"H\"], 4),\n            \"Saturation Z-Score\": round(z_scores[\"S\"], 4),\n            \"Value Z-Score\": round(z_scores[\"V\"], 4),\n            \"Combined Z-Score\": round(z_scores[\"Combined\"], 4)\n        })\n\n# Convert the results to a DataFrame\ndf = pd.DataFrame(results)\n\n# Save the DataFrame to an Excel file\noutput_file = 'complexity_onepieces.xlsx'\ndf.to_excel(output_file, index=False)\n","block_group":"31bef8106c3d4646bece1997f1bfb4c1","execution_count":2,"outputs":[{"name":"stdout","text":"Results saved to 1411_hsv_entropy_z_scores_results_bOberteil.xlsx\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/266d606f-9b82-4a2b-a87e-edb2bb47d7cf","content_dependencies":null},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"deepnote_app_block_visible":false,"cell_id":"a81d021f5005493b99d2f10709dd9e0d","deepnote_cell_type":"text-cell-h1"},"source":"# Scrape Images","block_group":"cc03d8a5f26b46a5a3a1a68ce5ecc746"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1724158432365,"execution_millis":2444,"deepnote_to_be_reexecuted":false,"cell_id":"6742cf870529408aa74e1ec6eb55db6b","deepnote_cell_type":"code"},"source":"pip install apify-client","block_group":"7d63304329964f9c9ebec1bfe1f64509","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1721982001762,"execution_millis":45627,"deepnote_to_be_reexecuted":true,"cell_id":"59fc5a406f0e4a17946f78322d3907c5","deepnote_cell_type":"code"},"source":"\nimport os\nimport json\nfrom apify_client import ApifyClient\n\n# Initialize the Apify client with your API token\napi_token = 'apify_api_bXONqF5EhaKgNAHDvCiHv2zc0J7zke2o55D3'\nclient = ApifyClient(api_token)\n\n# Define the input configuration for the scraper\ninput_data = {\n    \"searchUrl\": \"https://www.amazon.de/s?k=snowsuit+kids&crid=QVL4VLTFLGLM&sprefix=snowsuit+%2Caps%2C181&ref=nb_sb_ss_pltr-xclick_2_9\",\n    \"maxItems\": 20,  # Adjust the number of items as needed\n    \"proxy\": {\n        \"useApifyProxy\": True,\n        \"apifyProxyGroups\": [\"RESIDENTIAL\"]\n    },\n}\n\n# Run the scraper\nrun = client.actor(\"curious_coder/amazon-scraper\").call(run_input=input_data)\n\n# Fetch the results\ndataset_id = run[\"defaultDatasetId\"]\nitems = client.dataset(dataset_id).list_items().items\n\n# JSON file path\njson_file_path = 'amazon_snowsuits.json'\n\n# Write the results to the JSON file\nwith open(json_file_path, mode='w', encoding='utf-8') as file:\n    json.dump(items, file, ensure_ascii=False, indent=4)\n\nprint(f\"Data saved to {json_file_path}\")\n","block_group":"818f50cd5feb448d923d9afb823f2706","execution_count":null,"outputs":[{"name":"stdout","text":"Number of items fetched: 0\nData saved to asos_skirts.json\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/dc4daa7b-32a2-4813-a519-c78e1b96e95d","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=9f956f77-27e9-4ead-b449-10d39e88f021' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2024-11-17T19:55:10.245Z"},"deepnote_notebook_id":"bf7dbafd6c4641f9be8c3785c3ba6016"}}